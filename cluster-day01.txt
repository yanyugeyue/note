张志刚
2013年，网络，系统管理，服务，安全，监控，集群，RHCE，RHCA
PYTHON
集群：
第一天：存储
第二天～三天：集群
第四天～五天：CEPH分布式文件系统
第六天：复习

开头、提高
提高阶段：原理
学过不等于掌握
掌握：基本上会配置，懂原理

存储方式：
DAS：直连式存储，如本地硬盘
NAS：网络附加存储，实际上就是NFS／SAMBA（CIFS）
SAN：存储区域网络。分成基于IP网络的IP SAN和基于光纤的FC SAN。

配置iSCSI
1、 准备三台虚机
[root@room8pc16 ~]# clone-auto7
2、 初始化
Node1.tedu.cn  192.168.4.1   iSCSI服务器
Node2.tedu.cn  192.168.4.2   mysql1
Node3.tedu.cn  192.168.4.3   mysql2
关闭防火墙、SELINUX，配置YUM
（1）启动虚拟机
[root@room8pc16 ~]# for  vm  in  rh7_node{1..3}
> do
> virsh  start  $vm
> done
（2） 在物理机上连接虚拟机的控制台
[root@room8pc16 ~]# virsh  console  rh7_node1
退出时，按ctrl+]
[root@localhost ~]# nmtui
3、 iSCSI提供块设备
块设备，如硬盘、光盘、U盘
4、 iSCSI服务器端为客户端提供块设备，需要起个名，这个名字是IQN
IQN名称规范：iqn.时间.域名的反写.字符串:子字符串
如: iqn.2018-07.cn.tedu.nsd1803:yezhikang
5、 在iSCSI服务器上添加额外硬盘
可以在虚拟机不关机的情况下，直接添加硬盘。为node1加一块20GB的硬盘。

配置iSCSI服务端
1、 安装软件包
[root@node1 ~]# yum  install  -y  targetcli
2、 为vdb分区
MBR: 主引导记录，这种分区格式，最多只能支持4个分区，最大支持2.2TB左右硬盘
GPT：GUID分区表，可以支持大硬盘，可以支持128个主分区
[root@node1 ~]# parted  /dev/vdb 
(parted) mklabel  gpt
(parted) mkpart  primary  1M  50%
(parted) mkpart  primary  50%  100%
(parted) quit  
[root@node1 ~]# lsblk 
3、 配置iscsi
[root@node1 ~]# targetcli 
给vdb1起个名字formysql，回入到iscsi管理中
/> /backstores/block  create  formysql  /dev/vdb1 
创建供客户端访问的iqn名字
/> /iscsi  create  iqn.2018-07.cn.tedu.nsd1803
把存储绑定到iqn名字中
/> /iscsi/iqn.2018-07.cn.tedu.nsd1803/tpg1/luns create /backstores/block/formysql 
配置ACL，限定允许访问的客户端
/> /iscsi/iqn.2018-07.cn.tedu.nsd1803/tpg1/acls create iqn.2018-07.cn.tedu.node2
/> /iscsi/iqn.2018-07.cn.tedu.nsd1803/tpg1/acls create iqn.2018-07.cn.tedu.node3
保存并退出
/> saveconfig 
/> exit

配置node2为客户端
1、 安装软件包
[root@node2 ~]# yum  install  -y  iscsi-initiator-utils
2、 客户端修改iqn名字
[root@node2 ~]# vim /etc/iscsi/initiatorname.iscsi 
InitiatorName=iqn.2018-07.cn.tedu.node2
3、 发现存储端，此时可以发现名字，但是不会多出硬盘
[root@node2 ~]# iscsiadm --mode discoverydb --type sendtargets --portal 192.168.4.1 --discover
4、 登陆iscsi设备，出现新硬盘
[root@node2 ~]# systemctl  restart  iscsi
[root@node2 ~]# lsblk    ＃ 将会出现sda
[root@node2 ~]# systemctl enable iscsi
[root@node2 ~]# systemctl enable iscsid
iscsi服务用于自动登陆，iscsid是守护进程
5、 安装mariadb-server，将sda分区、格式化，挂载到mariadb的工作目录下
[root@node2 ~]# mount  /dev/sda1  /var/lib/mysql/
[root@node2 ~]# chown mysql.mysql /var/lib/mysql/
6、 启动mariadb，创建库和表
MariaDB [(none)]> create database nsd1803;
MariaDB [(none)]> use nsd1803;
MariaDB [nsd1803]> create table students (name varchar(20));
MariaDB [nsd1803]> INSERT INTO students VALUES('zhouyu');


iSCSI：
当node2节点出现故障时，可以使用node3进行替换
1、 配置iscsi客户端
[root@node3 ~]# vim /etc/iscsi/initiatorname.iscsi 
InitiatorName=iqn.2018-07.cn.tedu.node3
[root@node3 ~]# iscsiadm --mode discoverydb --type sendtargets --portal 192.168.4.1 --discover
[root@node3 ~]# systemctl restart iscsi
2、 配置mariadb-server
[root@node3 ~]# yum install -y mariadb-server
[root@node3 ~]# mount /dev/sda1 /var/lib/mysql/
[root@node3 ~]# systemctl start mariadb

注意：千万不要使用两个节点同时挂载相同的文件系统（分区），如果多个节点同时挂载这个文件系统，很可能会导致文件系统损坏，数据丢失！因为XFS／EXT3／EXT4这些文件系统都是单节点文件系统。红帽的GFS才能支持多节点同时挂载。

UDEV：动态管理硬件文件的方法。如，把U盘接入到主机，主机会多一个硬盘文件，把U盘移除的时候，硬盘文件消失。
1、 udev规则文件存放目录/etc/udev/rules.d/
2、 规则文件命名：数字-名字.rules
数字大小顺序是规则文件执行的顺序
3、 应用udev，给iscsi磁盘创建一个软链接叫idisk
（1） 查看iscsi磁盘（sda）在/sys/目录中的位置
[root@node3 ~]# udevadm info --query=path --name=/dev/sda1
（2） 通过上一步的路径，查看iscsi磁盘的信息（以和其他设备进行区别）
[root@node3 ~]# udevadm info --query=all --attribute-walk --path=/devices/platform/host2/session1/target2:0:0/2:0:0:0/block/sda/sda1
（3） 创建规则文件
[root@node3 ~]# vim /etc/udev/rules.d/90-iscsi.rules
KERNEL=="sd[a-z]*", ACTION=="add", SUBSYSTEMS=="scsi", ATTRS{model}=="formysql        ", SYMLINK+="idisk%n"
注：KERNEL==””表示内核识别出来的设备名
ACTION＝＝“add”表示新接入设备
SUBSYSTEMS和ATTRS{model}是第（2）步查到的
SYMLINK表示创建符号链接，+=表示额外创建，%n是分区号
（4） 只有把磁盘移除再接入才能生效或是重启系统
[root@node3 ~]# systemctl stop mariadb
[root@node3 ~]# umount /dev/sda1 
[root@node3 ~]# iscsiadm --mode node --targetname iqn.2018-07.cn.tedu.nsd1803 --portal 192.168.4.1:3260 --logout
[root@node3 ~]# iscsiadm --mode discoverydb --type sendtargets --portal 192.168.4.1 --discover
[root@node3 ~]# systemctl restart iscsi
[root@node3 ~]# ll  /dev/idisk*


NFS：实现网络共享的，用于unix-like（类unix）系统间的共享
端口号是2049，基于RPC（远程过程调用，端口号111）服务。NFS只提供了共享功能，底层数据传输交给RPC服务。
一、 只读共享
1、 node1作为服务端
（1）安装软件包
[root@node1 ~]# yum install -y nfs-utils
（2）创建共享目录
[root@node1 ~]# mkdir  -pv  /nfsroot/nfsro
[root@node1 ~]# cp /etc/hosts /nfsroot/nfsro
（3） 修改配置文件
[root@node1 ~]# vim /etc/exports
/nfsroot/nfsro  *(ro)   ->允许所有地址以只读方式访问
（4） 启动服务并验证
[root@node1 ~]# systemctl start nfs
[root@node1 ~]# showmount -e 192.168.4.1
2、 node3作为客户端
（1） 创建挂载点
[root@node3 ~]# mkdir /mnt/nsfshare
（2） 查看共享，并挂载
[root@node3 ~]# showmount  -e  192.168.4.1
[root@node3 ~]# mount 192.168.4.1:/nfsroot/nfsro /mnt/nsfshare
[root@node3 ~]# ls /mnt/nsfshare
二、 读写共享
不管是NFS，还是SAMBA，还是FTP，只要是读写共享，必须注意本地权限和配置文件内的授权
1、 Node1作为服务器
（1）创建用于读写目录
[root@node1 ~]# mkdir -m 777 /nfsroot/nfsrw
（2）修改配置文件说明
[root@node1 ~]# vim /etc/exports
/nfsroot/nfsro  *(ro)
/nfsroot/nfsrw  192.168.4.*(rw,sync)
（3）重新输出共享
[root@node1 ~]# exportfs  -rv
2、 配置客户端
（1） 创建挂载点
[root@node3 ~]# mkdir /mnt/rwnfs
（2）挂载，测试
[root@node3 ~]# mount 192.168.4.1:/nfsroot/nfsrw /mnt/rwnfs
[root@node3 ~]# echo 'hello world' > /mnt/rwnfs/hi.txt
3、 NFS选项
(1)no_root_squash表示当root创建文件时，保留文件的属主属组还是root，默认写入到共享目录中的文件属主属组是nfsnobody
/nfsroot/nfsrw  192.168.4.*(rw,sync,no_root_squash)
(2)all_squash：作用是客户端任何用户写入的文件属主属组都是nfsnobody。

多路径：当主机到共享存储有多条路径，每条路径都会被识别为一个设备。多路径配置可以将多个设备合并成一个虚拟设备。
1、 为node1和node3再配置一个网络192.168.2.0/24
# nmtui
# ifup  eth1
2、 node3在192.168.2.0网络上发现共享存储
[root@node3 ~]# iscsiadm --mode discoverydb --type sendtargets --portal 192.168.2.1 --discover
[root@node3 ~]# systemctl restart iscsi
[root@node3 ~]# lsblk   发现新的sdb
3、 主机通过WWID（全球识别符）来判断哪些路径上的设备是同一设备
[root@node3 ~]# /lib/udev/scsi_id --whitelisted --device=/dev/sda
360014059c375e9ab0604771aa3f719f1
4、 安装多路径设备软件
[root@node3 ~]# yum install -y device-mapper-multipath
5、 合并出来的多路径设备，一般起名为mpath[a-z]
6、 配置多路径
（1） 生成配置文件，不使用用户友好名称
[root@node3 ~]# mpathconf  --user_friendly_names  n
（2） 修改配置文件
[root@node3 ~]# vim /etc/multipath.conf 尾部追加
multipaths {
    multipath {
        wwid    "360014059c375e9ab0604771aa3f719f1"
        alias   mpatha
    }
}
（3） 多路径设备出现在/dev/mapper/目录下
[root@node3 ~]# ls /dev/mapper/
[root@node3 ~]# systemctl start multipathd
[root@node3 ~]# ls /dev/mapper/
[root@node3 ~]# mount /dev/mapper/mpatha1 /var/lib/mysql/
[root@node3 ~]# ls /var/lib/mysql/
[root@node3 ~]# multipath -ll   查看多路径信息





